\documentclass[12pt,a4paper]{article}
\usepackage[colorlinks=true, allcolors=black]{hyperref}
\usepackage[margin=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{enumitem}

\definecolor{codebg}{rgb}{0.95,0.95,0.95}
\definecolor{keywordcolor}{rgb}{0.0,0.4,0.8}

\lstset{
    backgroundcolor=\color{codebg},
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    rulecolor=\color{gray},
    keywordstyle=\color{keywordcolor}\bfseries,
}

\setlength{\fboxsep}{0pt}

\begin{document}
\begin{titlepage}
    \centering
    \rule{\textwidth}{1pt}\par
    \vspace*{1.2cm}
    {\Huge\textbf{Mobile Artificial Intelligence Distribution Manual}}\\
    \vspace*{1cm}
    \rule{\textwidth}{1pt}\par
    \vspace*{\fill}
    \includegraphics[width=0.4\textwidth]{./logo.png}\\
    \vspace*{\fill}
    \rule{\textwidth}{1pt}\par
    \vspace*{1cm}
    \makebox[\textwidth]{
        \large v3.0.0\hfill \today
    }
\end{titlepage}

% Table of contents
\tableofcontents

\newpage

% ─────────────────────────────────────────────────────────────────────────────
\section{Introduction}

Maid (Mobile Artificial Intelligence Distribution) is a free and open-source application for interfacing with AI language models on Android. It supports both fully local inference via \texttt{llama.cpp} and remote inference through the APIs of Anthropic, DeepSeek, Mistral, Ollama, and OpenAI.

All settings are stored locally on your device using encrypted application storage — no API keys or personal data are sent anywhere other than the provider you configure.

\subsection{Companion App}

For text-to-speech functionality, Maid has a companion app called \textbf{Maise}. It can be found at\\
\url{https://github.com/Mobile-Artificial-Intelligence/maise}.

% ─────────────────────────────────────────────────────────────────────────────
\section{Large Language Model Inference Providers}

\subsection{Feature Comparison}

\begin{center}
\begin{tabular}{lccccc}
\toprule
\textbf{Provider} & \textbf{API Key} & \textbf{Custom Base URL} & \textbf{Custom Headers} & \textbf{Local} \\
\midrule
Llama    & No  & No  & No  & Yes \\
Ollama   & No  & Yes & Yes & Yes \\
OpenAI   & Yes & Yes & Yes & No  \\
Anthropic & Yes & Yes & Yes & No  \\
Mistral  & Yes & Yes & No  & No  \\
DeepSeek & Yes & No  & Yes & No  \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Selecting a Provider}

On the Settings screen, tap the \textbf{API} dropdown at the top of the page to choose which AI provider Maid should use. The available options are:

\begin{itemize}[noitemsep]
    \item \textbf{Llama} — Local inference on-device (no internet required)
    \item \textbf{Ollama} — Local or self-hosted server
    \item \textbf{OpenAI} — OpenAI cloud API (and compatible endpoints)
    \item \textbf{Anthropic} — Anthropic Claude cloud API
    \item \textbf{Mistral} — Mistral AI cloud API
    \item \textbf{DeepSeek} — DeepSeek cloud API
\end{itemize}

After selecting a provider, the settings panel will update to show the fields relevant to that provider.

% ─────────────────────────────────────────────────────────────────────────────
\section{Llama (Local Inference)}

\subsection{Overview}

The Llama provider runs AI models directly on your device using \texttt{llama.cpp} via the \texttt{llama.rn} library. No internet connection is required and no data leaves your device. Performance depends on your device's CPU and available RAM.

\subsection{Selecting a Built-in Model}

Maid ships with a curated catalogue of GGUF models that can be downloaded directly from within the app. Open Settings, ensure \textbf{Llama} is selected as the API, then tap the \textbf{Model} dropdown to browse available models. Selecting a model and quantisation will begin the download automatically.

The built-in catalogue includes the following models:

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Parameters} & \textbf{Quantisations Available} \\
\midrule
LFM 2.5 1.2B Thinking      & 1.2B & Q4\_0, Q4\_K\_M, Q5\_K\_M, Q6\_K, Q8\_0, BF16, F16 \\
Qwen3 4B                   & 4.0B & Q4\_K\_M, Q5\_0, Q5\_K\_M, Q6\_K, Q8\_0 \\
Phi 3 Mini 4K Instruct     & 3.8B & Q4, FP16 \\
TinyLlama 1.1B Chat        & 1.1B & Q2\_K through Q8\_0 \\
Gemma 2 2B IT              & 2.0B & IQ3\_M through F32 \\
Gemma 3 1B IT              & 1.0B & Q2\_K through BF16 \\
Gemmasutra Mini 2B v1      & 2.0B & Q2\_K through F32 \\
Gemmasutra Small 4B v1a    & 4.0B & Q2\_K through Q8\_0 \\
Qwen2.5 1.5B Instruct      & 1.5B & Q2\_K through FP16 \\
Llama 3.2 1B Instruct      & 1.0B & IQ3\_M through F16 \\
Llama 3.2 3B Instruct      & 3.0B & IQ3\_M through F16 \\
Tesslate Tessa T1 3B       & 3.0B & IQ2\_M through BF16 \\
\bottomrule
\end{tabular}
\end{center}

\subsubsection{Choosing a Quantisation}

Quantisation reduces model size at the cost of some accuracy. A general guide:

\begin{itemize}[noitemsep]
    \item \textbf{Q2\_K / Q3\_K} — Smallest size, lowest quality. Useful when RAM is very limited.
    \item \textbf{Q4\_K\_M} — Good balance of size and quality. Recommended for most devices.
    \item \textbf{Q5\_K\_M / Q6\_K} — Higher quality, larger download. Suitable for higher-end devices.
    \item \textbf{Q8\_0} — Near-lossless quality. Requires the most RAM.
    \item \textbf{F16 / BF16} — Full precision. Not recommended for mobile inference.
\end{itemize}

\subsection{Loading a Custom Model File}

To use a GGUF model file you have obtained separately:

\begin{enumerate}
    \item Transfer the \texttt{.gguf} file to your device (e.g.\ via USB or a file manager).
    \item In Maid, go to \textbf{Settings} and select \textbf{Llama} as the API.
    \item Tap \textbf{Add Model File} and navigate to the file location using the file picker.
    \item Once added, select the file from the \textbf{Model} dropdown.
\end{enumerate}

\subsection{Configuration}

No API key or server address is needed. The only configurable items are:

\begin{itemize}[noitemsep]
    \item \textbf{Model} — The GGUF model file to load.
    \item \textbf{Parameters} — Optional inference parameters (see Section~\ref{sec:parameters}).
\end{itemize}

% ─────────────────────────────────────────────────────────────────────────────
\section{Ollama}

\subsection{Overview}

Ollama is a tool for running large language models locally on a desktop or server. Maid connects to an Ollama instance over the network, which means you can run a capable model on a more powerful machine (such as a desktop PC or a home server) and use Maid on your phone as the interface.

\subsection{Installing Ollama}

Install Ollama on the host machine by following the official instructions at \url{https://ollama.com}. Once installed, start the Ollama service and pull a model, for example:

\begin{lstlisting}[language=bash]
ollama pull llama3.2
\end{lstlisting}

By default Ollama listens on \texttt{http://localhost:11434}.

\subsection{Allowing Remote Connections}

If Maid is running on a different device than Ollama, you must allow Ollama to accept connections from other hosts. Set the environment variable before starting Ollama:

\begin{lstlisting}[language=bash]
OLLAMA_HOST=0.0.0.0 ollama serve
\end{lstlisting}

Ensure your firewall allows inbound connections on port \texttt{11434} from your phone's network.

\subsection{Configuration in Maid}

\begin{enumerate}
    \item Go to \textbf{Settings} and select \textbf{Ollama} from the API dropdown.
    \item Enter the \textbf{Base URL} of your Ollama server, e.g.:
\begin{lstlisting}
http://192.168.1.100:11434
\end{lstlisting}
    \item Maid will automatically fetch the list of models available on your Ollama server.
    \item Select a model from the \textbf{Model} dropdown.
    \item Optionally configure \textbf{Custom Headers} and \textbf{Parameters} (see Sections~\ref{sec:headers} and~\ref{sec:parameters}).
\end{enumerate}

\subsection{Required Fields}

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Field} & \textbf{Required} & \textbf{Default} \\
\midrule
Base URL & Yes & — \\
Model    & Yes & — \\
\bottomrule
\end{tabular}
\end{center}

% ─────────────────────────────────────────────────────────────────────────────
\section{OpenAI}

\subsection{Overview}

The OpenAI provider connects to OpenAI's cloud API. It also supports any third-party API that is compatible with the OpenAI REST specification, such as LM Studio, vLLM, or OpenRouter, by changing the Base URL.

\subsection{Obtaining an API Key}

\begin{enumerate}
    \item Create an account at \url{https://platform.openai.com}.
    \item Navigate to \textbf{API Keys} in your account dashboard.
    \item Click \textbf{Create new secret key} and copy the generated key.
\end{enumerate}

Keep your API key secure. Charges are incurred based on token usage.

\subsection{Configuration in Maid}

\begin{enumerate}
    \item Go to \textbf{Settings} and select \textbf{OpenAI} from the API dropdown.
    \item Paste your API key into the \textbf{API Key} field.
    \item Maid will automatically fetch and populate the \textbf{Model} dropdown with the models available on your account.
    \item Select a model (e.g.\ \texttt{gpt-4o}, \texttt{gpt-4o-mini}, \texttt{o3-mini}).
    \item Optionally set a \textbf{Custom Base URL} if you are using a compatible third-party endpoint.
    \item Optionally configure \textbf{Custom Headers} and \textbf{Parameters} (see Sections~\ref{sec:headers} and~\ref{sec:parameters}).
\end{enumerate}

\subsection{Required Fields}

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Field} & \textbf{Required} & \textbf{Default} \\
\midrule
API Key  & Yes & — \\
Model    & Yes & — \\
Base URL & No  & \texttt{https://api.openai.com/v1} \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Using Compatible APIs}

Because the Base URL is configurable, OpenAI mode also works with any OpenAI-compatible endpoint. Examples:

\begin{itemize}[noitemsep]
    \item \textbf{LM Studio} — set Base URL to \texttt{http://<host>:1234/v1}
    \item \textbf{OpenRouter} — set Base URL to \texttt{https://openrouter.ai/api/v1} and use your OpenRouter API key
    \item \textbf{vLLM} — set Base URL to \texttt{http://<host>:8000/v1}
\end{itemize}

% ─────────────────────────────────────────────────────────────────────────────
\section{Anthropic}

\subsection{Overview}

The Anthropic provider connects to Anthropic's Claude family of models via the official Claude API. Claude models are known for their strong reasoning, instruction-following, and safety characteristics.

\subsection{Obtaining an API Key}

\begin{enumerate}
    \item Create an account at \url{https://console.anthropic.com}.
    \item Navigate to \textbf{API Keys} in the left-hand sidebar.
    \item Click \textbf{Create Key}, name it, and copy the generated key.
\end{enumerate}

API usage is billed per token. Review Anthropic's pricing page for current rates.

\subsection{Configuration in Maid}

\begin{enumerate}
    \item Go to \textbf{Settings} and select \textbf{Anthropic} from the API dropdown.
    \item Paste your API key into the \textbf{API Key} field.
    \item Maid will automatically fetch and populate the \textbf{Model} dropdown with available Claude models (e.g.\ \texttt{claude-opus-4-6}, \texttt{claude-sonnet-4-6}, \texttt{claude-haiku-4-5}).
    \item Select a model.
    \item Optionally set a \textbf{Custom Base URL} if routing through a proxy.
    \item Optionally configure \textbf{Custom Headers} and \textbf{Parameters} (see Sections~\ref{sec:headers} and~\ref{sec:parameters}).
\end{enumerate}

\subsection{Required Fields}

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Field} & \textbf{Required} & \textbf{Default} \\
\midrule
API Key  & Yes & — \\
Model    & Yes & — \\
Base URL & No  & \texttt{https://api.anthropic.com} \\
\bottomrule
\end{tabular}
\end{center}

% ─────────────────────────────────────────────────────────────────────────────
\section{Mistral}

\subsection{Overview}

The Mistral provider connects to Mistral AI's cloud API. Mistral offers a range of efficient and capable models including Mistral Large, Mistral Small, and Codestral.

\subsection{Obtaining an API Key}

\begin{enumerate}
    \item Create an account at \url{https://console.mistral.ai}.
    \item Navigate to \textbf{API Keys} in the left-hand menu.
    \item Click \textbf{Create new key} and copy the generated key.
\end{enumerate}

\subsection{Configuration in Maid}

\begin{enumerate}
    \item Go to \textbf{Settings} and select \textbf{Mistral} from the API dropdown.
    \item Paste your API key into the \textbf{API Key} field.
    \item Maid will automatically fetch and populate the \textbf{Model} dropdown with available Mistral models.
    \item Select a model (e.g.\ \texttt{mistral-large-latest}, \texttt{mistral-small-latest}).
    \item Optionally set a \textbf{Custom Base URL} if using a self-hosted Mistral-compatible server.
    \item Optionally configure \textbf{Parameters} (see Section~\ref{sec:parameters}).
\end{enumerate}

\textbf{Note:} The Mistral provider does not support custom HTTP headers.

\subsection{Required Fields}

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Field} & \textbf{Required} & \textbf{Default} \\
\midrule
API Key  & Yes & — \\
Model    & Yes & — \\
Base URL & No  & \texttt{https://api.mistral.ai} \\
\bottomrule
\end{tabular}
\end{center}

% ─────────────────────────────────────────────────────────────────────────────
\section{DeepSeek}

\subsection{Overview}

The DeepSeek provider connects to the DeepSeek cloud API. DeepSeek offers high-quality models at competitive pricing, including the DeepSeek-V3 and DeepSeek-R1 reasoning model.

\subsection{Obtaining an API Key}

\begin{enumerate}
    \item Create an account at \url{https://platform.deepseek.com}.
    \item Navigate to \textbf{API Keys} in your dashboard.
    \item Click \textbf{Create API Key} and copy the generated key.
\end{enumerate}

\subsection{Configuration in Maid}

\begin{enumerate}
    \item Go to \textbf{Settings} and select \textbf{DeepSeek} from the API dropdown.
    \item Paste your API key into the \textbf{API Key} field.
    \item Maid will automatically fetch and populate the \textbf{Model} dropdown.
    \item Select a model (e.g.\ \texttt{deepseek-chat}, \texttt{deepseek-reasoner}).
    \item Optionally configure \textbf{Custom Headers} and \textbf{Parameters} (see Sections~\ref{sec:headers} and~\ref{sec:parameters}).
\end{enumerate}

\textbf{Note:} The DeepSeek provider always connects to \texttt{https://api.deepseek.com}. The base URL is not configurable.

\subsection{Required Fields}

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Field} & \textbf{Required} & \textbf{Default} \\
\midrule
API Key & Yes & — \\
Model   & Yes & — \\
\bottomrule
\end{tabular}
\end{center}

% ─────────────────────────────────────────────────────────────────────────────
\section{Prompt Input}

The prompt input bar is located at the bottom of the chat screen. It consists of a multiline text field and a context-sensitive action button on the right.

\subsection{Text Field}

Tap the text field to open the keyboard and type a message. The field grows vertically as you type to accommodate longer messages. While the field is empty its placeholder reads \emph{``Type a message\ldots''}.

\subsection{Clear Prompt}

When the text field contains any text, a \textbf{Clear Prompt} link appears above the input bar. Tapping it instantly empties the field.

\subsection{Action Button}

The button on the right of the input bar changes depending on the current state:

\begin{itemize}[noitemsep]
    \item \textbf{Send} — Shown when the field contains text. Tap to submit the message and start a model response. The button is disabled if the selected provider is not ready (e.g.\ no model loaded or no API key set).
    \item \textbf{Stop} — Shown while the model is generating a response. Tap to cancel the in-progress generation.
    \item \textbf{Microphone} — Shown when the field is empty and the model is not generating. Tap to begin voice dictation (see Section~\ref{sec:dictation}).
    \item \textbf{Microphone off} — Shown while voice dictation is active. Tap to stop listening; the transcribed text is appended to the field.
\end{itemize}

\subsection{Sending a Message}

Type your message and tap \textbf{Send} (or tap the microphone to dictate it). Maid appends a user message node and an empty assistant message node to the conversation tree, then streams the model response into the assistant node in real time. The input field is cleared immediately after the message is submitted.

If this is the first message in a new chat, a system prompt node is created automatically using the system prompt configured in Settings (defaulting to \emph{``You are a helpful assistant.''}).

\subsection{Voice Dictation}
\label{sec:dictation}

Tap the \textbf{microphone} icon to dictate your message by voice. Maid will request microphone permission on first use; if permission has been permanently denied, dictation will not start. Once listening begins, the button switches to a \textbf{microphone off} icon which you can tap to stop early. When recognition finishes, the transcript is appended to any text already in the field, with a space inserted between existing text and the new transcript.

Dictation uses the device's on-board speech recognition engine and is performed in English (en-US) with punctuation added automatically.

% ─────────────────────────────────────────────────────────────────────────────
\section{Messages}

Each message in the chat view has a row of controls displayed in the top-right corner of the message. The controls available depend on whether the message is from the user or the assistant.

\subsection{Branch Navigation}

Every message in Maid is part of a tree rather than a flat list. When a response is regenerated or a user message is edited, a new branch is created rather than overwriting the existing message. The branch navigator lets you move between these branches.

\begin{itemize}[noitemsep]
    \item \textbf{Left arrow} (\texttt{menu-left}) — Switch to the previous sibling of this message.
    \item \textbf{Counter} — Displays the current branch position as \emph{current} / \emph{total} (e.g.\ \texttt{2 / 3}).
    \item \textbf{Right arrow} (\texttt{menu-right}) — Switch to the next sibling of this message.
\end{itemize}

The arrows are disabled when there is only one branch, when a message is being edited, or while the model is generating a response.

\subsection{Delete}

The \textbf{delete} (trash) icon is available on all messages. Tapping it removes the message and all of its descendants from the conversation tree. This action cannot be undone, so use it with care.

Delete is disabled while a message is being edited or while the model is busy.

\subsection{Assistant Message Controls}

The following controls are shown on \textbf{assistant} messages only.

\subsubsection{Regenerate}

Tap the \textbf{reload} icon to regenerate an assistant message. A new branch is created under the parent user message and the model is prompted again with the current conversation up to that point, using the active provider and parameters. The existing response is preserved and accessible via the branch navigator.

Regeneration is disabled while another message is being edited or while the model is busy.

\subsubsection{Text-to-Speech}

If a voice has been selected in Settings, a speaker icon appears on assistant messages. The active voice can be changed at any time from the \textbf{Voice} option on the Settings screen. Available voices include those provided by system TTS engines such as Google Text-to-Speech (pre-installed on most Android devices) as well as voices from the Maise companion app.

\begin{itemize}[noitemsep]
    \item \textbf{Volume high} icon — Tap to read the assistant message aloud using the selected voice. If the message contains a reasoning block, only the final response (not the internal reasoning) is spoken.
    \item \textbf{Volume off} icon — Shown while speech is playing. Tap to stop playback immediately.
\end{itemize}

TTS is disabled while a message is being edited or while the model is generating a response.

\subsubsection{Feedback (Upvote / Downvote)}

Once an assistant message has finished generating, a pair of thumbs icons appears at the bottom of the message content.

\begin{itemize}[noitemsep]
    \item \textbf{Thumbs up} — Marks the response as good. The message content, provider name, and model name are submitted to the developer as a positive report.
    \item \textbf{Thumbs down} — Marks the response as poor. The same information is submitted as a negative report.
\end{itemize}

Reports are submitted anonymously via Supabase. If no existing session is found, an anonymous sign-in is performed automatically before the report is inserted. A confirmation dialog is shown on success, or an error alert if the submission fails.

No personally identifying information beyond the anonymous session identifier is attached to the report. The purpose of these reports is to help the developer assess model quality across different providers and configurations.

\subsection{User Message Controls}

The following control is shown on \textbf{user} messages only.

\subsubsection{Edit}

Tap the \textbf{pencil} icon to edit a user message. The message content becomes editable in-place. Submitting the edited message creates a new branch from the parent of the original message, preserving the original text in the previous branch. The branch navigator can then be used to switch between the original and the edited version.

Editing is disabled while another message is already being edited or while the model is generating.


% ─────────────────────────────────────────────────────────────────────────────
\section{Advanced Configuration}

\subsection{Model Parameters}
\label{sec:parameters}

All providers expose a \textbf{Parameters} panel that lets you fine-tune inference behaviour. Tap \textbf{Parameters} on the Settings screen to expand the panel. Common parameters include:

\begin{center}
\begin{tabular}{lp{9cm}}
\toprule
\textbf{Parameter} & \textbf{Description} \\
\midrule
\texttt{temperature} & Controls randomness. Lower values (e.g.\ 0.2) produce more deterministic output; higher values (e.g.\ 1.0) produce more varied output. \\
\texttt{top\_p}      & Nucleus sampling threshold. Limits token selection to the top $p$ cumulative probability mass. \\
\texttt{top\_k}      & Limits token selection to the top $k$ most likely tokens at each step. \\
\texttt{max\_tokens} & Maximum number of tokens the model may generate in a single response. \\
\texttt{seed}        & Sets a fixed random seed for reproducible outputs (where supported). \\
\bottomrule
\end{tabular}
\end{center}

Parameters that are left blank are omitted from the API request, allowing each provider to use its own defaults.

\subsection{Custom HTTP Headers}
\label{sec:headers}

The OpenAI, Anthropic, DeepSeek, and Ollama providers allow you to supply additional HTTP headers. This is useful for:

\begin{itemize}[noitemsep]
    \item Passing organisation or project identifiers required by a corporate proxy.
    \item Supplying additional authentication tokens (e.g.\ \texttt{HTTP-Referer} required by OpenRouter).
    \item Tagging requests for logging or rate-limit tier purposes.
\end{itemize}

To add a header, tap \textbf{Headers} on the Settings screen, then tap the \textbf{+} button. Enter the header name and value, then save. Multiple headers can be added.

% ─────────────────────────────────────────────────────────────────────────────
\section{Signing Fingerprints}

To verify that an APK was signed by the official Maid developer, check its certificate fingerprints against the values listed here. A mismatch indicates the APK may have been tampered with or repackaged.

\subsection{Signing Key}

The release signing key fingerprints are:

\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Algorithm} & \textbf{Fingerprint} \\
\midrule
MD5    & \texttt{BE:AC:29:41:F5:41:D2:26:42:DD:D1:A3:85:21:E1:16} \\
\addlinespace
SHA-1  & \texttt{48:F6:DC:73:09:CE:19:C6:A9:70:7E:A2:9A:B7:6F:42:2D:41:32:30} \\
\addlinespace
SHA-256 & \texttt{83:5E:D2:2E:D8:95:C4:C2:72:D6:98:AA:6E:4E:48:DB:} \\
        & \texttt{0B:4E:36:DC:CF:70:10:D5:DE:15:03:4A:C9:E1:B9:6F} \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Upload Key (Google Play)}

APKs distributed through Google Play are re-signed by Google using the upload key. Its fingerprints are:

\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Algorithm} & \textbf{Fingerprint} \\
\midrule
MD5    & \texttt{C0:86:A0:F3:E8:E5:4D:46:60:8A:37:4E:DB:11:CC:C7} \\
\addlinespace
SHA-1  & \texttt{77:FC:77:2B:21:5E:9F:36:31:79:09:DF:7D:F4:1F:CA:96:0C:39:17} \\
\addlinespace
SHA-256 & \texttt{54:EE:B9:9F:14:38:D9:68:9B:C2:C6:7F:F9:DD:A3:E3:} \\
        & \texttt{D8:28:D3:80:76:46:B7:24:46:71:9F:61:D9:63:E6:98} \\
\bottomrule
\end{tabular}
\end{center}

\subsection{How to Verify}

To inspect the certificate of a locally built or sideloaded APK, run the following command using the Android SDK build tools:

\begin{lstlisting}[language=bash]
apksigner verify --print-certs maid.apk
\end{lstlisting}

Compare the printed SHA-256 fingerprint against the signing key value above. For APKs downloaded from Google Play, compare against the upload key fingerprint instead.

\end{document}
