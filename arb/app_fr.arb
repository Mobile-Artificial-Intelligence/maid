{
    "@@locale": "fr",
    "friendlyName": "Français",
    "localeTitle": "Langue",
    "defaultLocale": "Langue par défaut",
    "loading": "Chargement...",
    "loadModel": "Charger le modèle",
    "noModelSelected": "Aucun modèle sélectionné",
    "noModelLoaded": "Aucun modèle chargé",
    "delete": "Supprimer",
    "import": "Importer",
    "export": "Exporter",
    "edit": "Modifier",
    "regenerate": "Régénérer",
    "chatsTitle": "Discussions",
    "newChat": "Nouvelle discussion",
    "anErrorOccurred": "Une erreur est survenue",
    "errorTitle": "Erreur",
    "key": "Clé",
    "value": "Valeur",
    "ok": "OK",
    "done": "Terminé",
    "close": "Fermer",
    "save": "Enregistrer {label}",
    "next": "Suivant",
    "previous": "Précédent",
    "contentShared": "Contenu partagé",
    "setUserImage": "Définir l'image de l'utilisateur",
    "setAssistantImage": "Définir l'image de l'assistant",
    "loadUserImage": "Charger l'image de l'utilisateur",
    "loadAssistantImage": "Charger l'image de l'assistant",
    "userName": "Nom d'utilisateur",
    "assistantName": "Nom de l'assistant",
    "user": "Utilisateur",
    "assistant": "Assistant",
    "cancel": "Annuler",
    "aiEcosystem": "Écosystème IA",
    "llamaCpp": "Llama CPP",
    "llamaCppModel": "Modèle Llama CPP",
    "remoteModel": "Modèle distant",
    "refreshRemoteModels": "Rafraîchir les modèles distants",
    "ollama": "Ollama",
    "searchLocalNetwork": "Rechercher sur le réseau local",
    "localNetworkSearchTitle": "Recherche sur le réseau local",
    "localNetworkSearchContent": "Cette fonctionnalité nécessite des permissions supplémentaires pour rechercher des instances Ollama sur votre réseau local.",
    "openAI": "OpenAI",
    "mistral": "Mistral",
    "anthropic": "Anthropic",
    "gemini": "Gemini",
    "modelParameters": "Paramètres du modèle",
    "addParameter": "Ajouter un paramètre",
    "removeParameter": "Supprimer le paramètre",
    "saveParameters": "Enregistrer les paramètres",
    "importParameters": "Importer les paramètres",
    "exportParameters": "Exporter les paramètres",
    "selectAiEcosystem": "Sélectionner l'écosystème IA",
    "selectRemoteModel": "Sélectionner un modèle distant",
    "selectThemeMode": "Sélectionner le mode du thème",
    "themeMode": "Mode du thème",
    "themeModeSystem": "Système",
    "themeModeLight": "Clair",
    "themeModeDark": "Sombre",
    "themeSeedColor": "Couleur de base du thème",
    "editMessage": "Modifier le message",
    "settingsTitle": "Paramètres",
    "aiSettings": "Paramètres de {aiControllerType}",
    "userSettings": "Paramètres utilisateur",
    "assistantSettings": "Paramètres de l'assistant",
    "systemSettings": "Paramètres système",
    "systemPrompt": "Prompt système",
    "clearChats": "Effacer les discussions",
    "resetSettings": "Réinitialiser les paramètres",
    "clearCache": "Vider le cache",
    "aboutTitle": "À propos",
    "aboutContent": "Maid est une application multiplateforme gratuite et open-source permettant d'interagir avec les modèles Llama.cpp en local et à distance avec Ollama, Mistral, Google Gemini et OpenAI. Maid prend en charge les cartes de personnages Sillytavern pour interagir avec vos personnages préférés. Vous pouvez télécharger une liste de modèles sélectionnés directement depuis Huggingface. Maid est distribuée sous la licence MIT et est fournie sans aucune garantie, explicite ou implicite. Maid n'est affiliée à Huggingface, Meta (Facebook), MistralAI, OpenAI, Google, Microsoft ou toute autre entreprise proposant un modèle compatible avec cette application.",
    "leadMaintainer": "Responsable principal",
    "apiKey": "Clé API",
    "baseUrl": "URL de base",
    "clearPrompt": "Effacer le prompt",
    "submitPrompt": "Envoyer le prompt",
    "stopPrompt": "Arrêter le prompt",
    "typeMessage": "Tapez un message...",
    "code": "Code",
    "copyLabel": "Copier {label}",
    "labelCopied": "{label} copié dans le presse-papiers!"
}